DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(AddBlock(ReceivedBlockInfo(0,Some(10),None,BlockManagerBasedStoreResult(input-0-1463343410400,Some(10)))),true)
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Stream 0 received block input-0-1463343410400
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.117551 ms) AkkaMessage(AddBlock(ReceivedBlockInfo(0,Some(10),None,BlockManagerBasedStoreResult(input-0-1463343410400,Some(10)))),true) from Actor[akka://sparkDriver/temp/$Z7b]
DEBUG Thread-41 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Reported block input-0-1463343410400
INFO Thread-41 org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1463343410400
DEBUG cluster-ClusterId{value='5738d436e5203708e753335f', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d436e5203708e753335f', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.3 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3314}
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3314}
DEBUG Thread-1-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0323, packet:: clientPath:null serverPath:null finished:false header:: 548,5  replyHeader:: 548,17839,0  request:: '/consumers/testgroup/offsets/mytopic/0,#3130303431,-1  response:: s{97,17839,1463242096736,1463343410743,2534,0,0,0,5,0,97} 
DEBUG RecurringTimer - BlockGenerator org.apache.spark.streaming.util.RecurringTimer - Callback for BlockGenerator called at time 1463343410800
DEBUG cluster-ClusterId{value='5738d491e5203708e7533496', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3315}
DEBUG cluster-ClusterId{value='5738d491e5203708e7533496', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3315}
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mytopic
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,1] has leader 0
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mytopic are 0,1
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mytopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6532 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342040192-a08aa80e-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mytopic:0: fetched offset = 10042: consumed offset = 10042) to 10042
DEBUG mytopic-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mytopic:0: fetched offset = 10042: consumed offset = 10042 to 10042
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6532 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic sparktopic
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,1] has leader 0
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: sparktopic are 0,1
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [sparktopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6532 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAddCookies - CookieSpec selected: best-match
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAuthCache - Auth cache not set in the context
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection request: [route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection leased: [id: 0][route: {}->http://localhost:8888][total kept alive: 0; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Stale connection check
DEBUG ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (sparktopic:0: fetched offset = 8021: consumed offset = 8020) to 8021
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6532 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG KafkaMessageHandler-1 kafka.consumer.PartitionTopicInfo - reset consume offset of sparktopic:0: fetched offset = 8021: consumed offset = 8021 to 8021
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[read] I/O error: Read timed out"
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Executing request POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Target auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Proxy auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Length: 195
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Host: localhost:8888
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Connection: Keep-Alive
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> User-Agent: Apache-HttpClient/4.3.3 (java 1.5)
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Accept-Encoding: gzip,deflate
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "POST /api/v1/datapoints HTTP/1.1[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Length: 195[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Host: localhost:8888[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Connection: Keep-Alive[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "User-Agent: Apache-HttpClient/4.3.3 (java 1.5)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Accept-Encoding: gzip,deflate[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[{"name":"28-db-b1-1f-06-00-00-d3","tags":{"deviceType":"DS18B20","celciusTemperature":"31.31","temperature":"88.36","deviceId":"28-db-b1-1f-06-00-00-d3"},"datapoints":[[1463343410885,"88.36"]]}]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "HTTP/1.1 204 No Content[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Access-Control-Allow-Origin: *[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Pragma: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Cache-Control: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Expires: 0[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Server: Jetty(8.1.16.v20140903)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[\r][\n]"
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << HTTP/1.1 204 No Content
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Access-Control-Allow-Origin: *
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Pragma: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Cache-Control: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Expires: 0
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Server: Jetty(8.1.16.v20140903)
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Connection can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection [id: 0][route: {}->http://localhost:8888] can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection released: [id: 0][route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mongoDB
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,0] has leader 0
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,1] has leader 0
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mongoDB are 0,1
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mongoDB,0]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6532 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6532 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342041123-51e37853-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mongoDB:0: fetched offset = 6403: consumed offset = 6402) to 6403
DEBUG mongoDB-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mongoDB:0: fetched offset = 6403: consumed offset = 6403 to 6403
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {listCollections : BsonInt32{value=1}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {getMore : BsonInt64{value=7623574678}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Inserting 1 documents into namespace idea.devices on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Insert completed
DEBUG cluster-ClusterId{value='5738d3fbe5203708e753327b', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d3fbe5203708e753327c', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d3fbe5203708e753327b', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.2 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d3fbe5203708e753327c', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.2 ms, state=CONNECTED}]
DEBUG RecurringTimer - BlockGenerator org.apache.spark.streaming.util.RecurringTimer - Callback for BlockGenerator called at time 1463343411000
DEBUG RecurringTimer - JobGenerator org.apache.spark.streaming.util.RecurringTimer - Callback for JobGenerator called at time 1463343411000
DEBUG JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Got event GenerateJobs(1463343411000 ms)
INFO Thread-41 org.apache.spark.storage.MemoryStore - ensureFreeSpace(188) called with curMem=425954, maxMem=1030823608
DEBUG JobGenerator org.apache.spark.streaming.DStreamGraph - Generating jobs for time 1463343411000 ms
DEBUG JobGenerator org.apache.spark.streaming.dstream.FilteredDStream - Time 1463343411000 ms is valid
INFO Thread-41 org.apache.spark.storage.MemoryStore - Block input-0-1463343410800 stored as bytes in memory (estimated size 188.0 B, free 982.7 MB)
DEBUG JobGenerator org.apache.spark.streaming.dstream.MappedDStream - Time 1463343411000 ms is valid
DEBUG JobGenerator org.apache.spark.streaming.kafka.KafkaInputDStream - Time 1463343411000 ms is valid
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),input-0-1463343410800,StorageLevel(false, true, false, false, 2),188,0,0),true) from Actor[akka://sparkDriver/temp/$07b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),input-0-1463343410800,StorageLevel(false, true, false, false, 2),188,0,0),true)
INFO sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.storage.BlockManagerInfo - Added input-0-1463343410800 in memory on localhost:54845 (size: 188.0 B, free: 982.7 MB)
DEBUG Thread-41 org.apache.spark.storage.BlockManagerMaster - Updated info of block input-0-1463343410800
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.105065 ms) AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),input-0-1463343410800,StorageLevel(false, true, false, false, 2),188,0,0),true) from Actor[akka://sparkDriver/temp/$07b]
DEBUG Thread-41 org.apache.spark.storage.BlockManager - Told master about block input-0-1463343410800
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(GetLocations(input-0-1463343410400),true) from Actor[akka://sparkDriver/temp/$17b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(GetLocations(input-0-1463343410400),true)
DEBUG Thread-41 org.apache.spark.storage.BlockManager - Put block input-0-1463343410800 locally took  0 ms
DEBUG Thread-41 org.apache.spark.storage.BlockManager - Replicating input-0-1463343410800 of 188 bytes to 0 peer(s) took 0 ms
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.054422 ms) AkkaMessage(GetLocations(input-0-1463343410400),true) from Actor[akka://sparkDriver/temp/$17b]
WARN Thread-41 org.apache.spark.storage.BlockManager - Block input-0-1463343410800 replicated to only 0 peer(s) instead of 1 peers
DEBUG Thread-41 org.apache.spark.storage.BlockManager - Put block input-0-1463343410800 remotely took  0 ms
DEBUG Thread-41 org.apache.spark.storage.BlockManager - Putting block input-0-1463343410800 with replication took  0 ms
DEBUG Thread-41 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Pushed block input-0-1463343410800 in 1 ms
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1) +++
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(AddBlock(ReceivedBlockInfo(0,Some(1),None,BlockManagerBasedStoreResult(input-0-1463343410800,Some(1)))),true) from Actor[akka://sparkDriver/temp/$27b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(AddBlock(ReceivedBlockInfo(0,Some(1),None,BlockManagerBasedStoreResult(input-0-1463343410800,Some(1)))),true)
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.streaming.scheduler.ReceivedBlockTracker - Stream 0 received block input-0-1463343410800
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.070006 ms) AkkaMessage(AddBlock(ReceivedBlockInfo(0,Some(1),None,BlockManagerBasedStoreResult(input-0-1463343410800,Some(1)))),true) from Actor[akka://sparkDriver/temp/$27b]
DEBUG Thread-41 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Reported block input-0-1463343410800
INFO Thread-41 org.apache.spark.streaming.receiver.BlockGenerator - Pushed block input-0-1463343410800
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.serialVersionUID
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.api.java.function.Function org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.fun$1
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + declared methods: 1
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1.apply(java.lang.Object)
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + outer classes: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + outer objects: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + there are no enclosing objects!
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  +++ closure <function1> (org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction$1) is now cleaned +++
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1) +++
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.serialVersionUID
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.api.java.function.Function org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.f$1
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.apply(java.lang.Object)
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public final boolean org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.apply(java.lang.Object)
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + outer classes: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + outer objects: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + there are no enclosing objects!
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  +++ closure <function1> (org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1) is now cleaned +++
DEBUG JobGenerator org.apache.spark.streaming.dstream.FilteredDStream - Time 1463343411000 ms is valid
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner - +++ Cleaning closure <function1> (org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1) +++
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + declared fields: 2
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public static final long org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.serialVersionUID
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      private final org.apache.spark.api.java.function.Function org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.f$1
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + declared methods: 2
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public final java.lang.Object org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.apply(java.lang.Object)
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -      public final boolean org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1.apply(java.lang.Object)
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + inner classes: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + outer classes: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + outer objects: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + populating accessed fields because this is the starting closure
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + fields accessed by starting closure: 0
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  + there are no enclosing objects!
DEBUG JobGenerator org.apache.spark.util.ClosureCleaner -  +++ closure <function1> (org.apache.spark.streaming.api.java.JavaDStream$$anonfun$filter$1) is now cleaned +++
DEBUG JobGenerator org.apache.spark.streaming.DStreamGraph - Generated 2 jobs for time 1463343411000 ms
INFO JobGenerator org.apache.spark.streaming.scheduler.JobScheduler - Added jobs for time 1463343411000 ms
DEBUG JobGenerator org.apache.spark.streaming.scheduler.JobGenerator - Got event DoCheckpoint(1463343411000 ms,false)
INFO Thread-9 org.apache.spark.streaming.StreamingContext - Invoking stop(stopGracefully=false) from shutdown hook
DEBUG Thread-9 org.apache.spark.streaming.scheduler.JobScheduler - Stopping JobScheduler
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(StopAllReceivers,true) from Actor[akka://sparkDriver/temp/$37b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(StopAllReceivers,true)
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(org.apache.spark.streaming.receiver.StopReceiver$@4600b781,false) from Actor[akka://sparkDriver/deadLetters]
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(org.apache.spark.streaming.receiver.StopReceiver$@4600b781,false)
INFO sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.streaming.scheduler.ReceiverTracker - Sent stop signal to all 1 receivers
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (4.00872 ms) AkkaMessage(StopAllReceivers,true) from Actor[akka://sparkDriver/temp/$37b]
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Received stop signal
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopping receiver with message: Stopped by driver: 
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], ZKConsumerConnector shutting down
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.utils.KafkaScheduler - Shutting down task scheduler.
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ConsumerFetcherManager - [ConsumerFetcherManager-1463342046219] Stopping leader finder thread
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ConsumerFetcherManager$LeaderFinderThread - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-leader-finder-thread], Shutting down
INFO testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-leader-finder-thread kafka.consumer.ConsumerFetcherManager$LeaderFinderThread - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-leader-finder-thread], Stopped 
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ConsumerFetcherManager$LeaderFinderThread - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-leader-finder-thread], Shutdown completed
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ConsumerFetcherManager - [ConsumerFetcherManager-1463342046219] Stopping all fetchers
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0], Shutting down
INFO ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0 kafka.consumer.SimpleConsumer - Reconnect due to socket error: java.nio.channels.ClosedByInterruptException
DEBUG ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0 kafka.consumer.SimpleConsumer - Disconnecting from localhost:9092
DEBUG ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0 kafka.consumer.SimpleConsumer - Disconnecting from localhost:9092
DEBUG ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0 kafka.consumer.SimpleConsumer - Disconnecting from localhost:9092
INFO ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0 kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0], Stopped 
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ConsumerFetcherThread - [ConsumerFetcherThread-testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be-0-0], Shutdown completed
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.SimpleConsumer - Disconnecting from localhost:9092
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ConsumerFetcherManager - [ConsumerFetcherManager-1463342046219] All connections stopped
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Clearing up queue
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Cleared queue and sent shutdown command
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Clearing up queue
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Cleared queue and sent shutdown command
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Clearing up queue
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Cleared queue and sent shutdown command
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Clearing up queue
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Cleared queue and sent shutdown command
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Clearing up queue
DEBUG sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], Cleared queue and sent shutdown command
DEBUG KafkaMessageHandler-2 kafka.consumer.ConsumerIterator - Received the shutdown command
DEBUG KafkaMessageHandler-3 kafka.consumer.ConsumerIterator - Received the shutdown command
DEBUG KafkaMessageHandler-1 kafka.consumer.ConsumerIterator - Received the shutdown command
DEBUG KafkaMessageHandler-0 kafka.consumer.ConsumerIterator - Received the shutdown command
DEBUG KafkaMessageHandler-4 kafka.consumer.ConsumerIterator - Received the shutdown command
DEBUG Executor task launch worker-0-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0321, packet:: clientPath:null serverPath:null finished:false header:: 39,5  replyHeader:: 39,17840,0  request:: '/consumers/testListener/offsets/sparktopic/0,#38303231,-1  response:: s{142,17840,1463242162886,1463343411072,66,0,0,0,4,0,142} 
DEBUG Executor task launch worker-0-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0321, packet:: clientPath:null serverPath:null finished:false header:: 40,5  replyHeader:: 40,17841,0  request:: '/consumers/testListener/offsets/sparktopic/1,#36363838,-1  response:: s{145,17841,1463242162890,1463343411073,70,0,0,0,4,0,145} 
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.I0Itec.zkclient.ZkClient - Closing ZkClient...
INFO ZkClient-EventThread-134-localhost org.I0Itec.zkclient.ZkEventThread - Terminate ZkClient event thread.
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.I0Itec.zkclient.ZkConnection - Closing ZooKeeper connected to localhost
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.zookeeper.ZooKeeper - Closing session: 0x154b0e6621f0321
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.zookeeper.ClientCnxn - Closing client for session: 0x154b0e6621f0321
DEBUG Executor task launch worker-0-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Got notification sessionid:0x154b0e6621f0321
DEBUG Executor task launch worker-0-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Got WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/consumers/testListener/ids for sessionid 0x154b0e6621f0321
DEBUG Executor task launch worker-0-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0321, packet:: clientPath:null serverPath:null finished:false header:: 41,-11  replyHeader:: 41,17842,0  request:: null response:: null
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.zookeeper.ClientCnxn - Disconnecting client for session: 0x154b0e6621f0321
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.zookeeper.ZooKeeper - Session: 0x154b0e6621f0321 closed
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.I0Itec.zkclient.ZkClient - Closing ZkClient...done
DEBUG Executor task launch worker-0-EventThread org.I0Itec.zkclient.ZkClient - Received event: WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/consumers/testListener/ids
DEBUG Executor task launch worker-0-EventThread org.I0Itec.zkclient.ZkClient - ignoring event '{NodeChildrenChanged | /consumers/testListener/ids}' since shutdown triggered
DEBUG Executor task launch worker-0-EventThread org.I0Itec.zkclient.ZkClient - Leaving process event
INFO Executor task launch worker-0-EventThread org.apache.zookeeper.ClientCnxn - EventThread shut down
INFO sparkDriver-akka.actor.default-dispatcher-3 kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], ZKConsumerConnector shutdown completed in 31 ms
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Called receiver onStop
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Deregistering receiver 0
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(DeregisterReceiver(0,Stopped by driver,),true) from Actor[akka://sparkDriver/temp/$47b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(DeregisterReceiver(0,Stopped by driver,),true)
ERROR sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.streaming.scheduler.ReceiverTracker - Deregistered receiver for stream 0: Stopped by driver
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (2.303937 ms) AkkaMessage(DeregisterReceiver(0,Stopped by driver,),true) from Actor[akka://sparkDriver/temp/$47b]
DEBUG cluster-ClusterId{value='5738d44be5203708e75333af', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d44be5203708e75333af', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.3 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d8e3e5203708e7533e16', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3316}
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver 0
DEBUG cluster-ClusterId{value='5738d8e3e5203708e7533e16', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3316}
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.BlockGenerator - Stopping BlockGenerator
DEBUG cluster-ClusterId{value='5738d423e5203708e7533310', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d423e5203708e7533310', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.3 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d405e5203708e753329a', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d405e5203708e7533299', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d405e5203708e7533299', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.2 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d405e5203708e753329a', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.2 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3317}
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3317}
DEBUG RecurringTimer - BlockGenerator org.apache.spark.streaming.util.RecurringTimer - Callback for BlockGenerator called at time 1463343411200
INFO testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be_watcher_executor kafka.consumer.ZookeeperConsumerConnector - [testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be], stopping watcher executor thread for consumer testListener_Surbhis-MacBook-Pro.local-1463342046215-c0f1d1be
DEBUG cluster-ClusterId{value='5738d419e5203708e75332e8', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d419e5203708e75332e8', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.3 ms, state=CONNECTED}]
DEBUG RecurringTimer - BlockGenerator org.apache.spark.streaming.util.RecurringTimer - Callback for BlockGenerator called at time 1463343411400
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.util.RecurringTimer - Stopped timer for BlockGenerator after time 1463343411400
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.BlockGenerator - Waiting for block pushing thread to terminate
INFO Thread-41 org.apache.spark.streaming.receiver.BlockGenerator - Pushing out the last 0 blocks
INFO Thread-41 org.apache.spark.streaming.receiver.BlockGenerator - Stopped block pushing thread
INFO sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.streaming.receiver.BlockGenerator - Stopped BlockGenerator
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (358.682599 ms) AkkaMessage(org.apache.spark.streaming.receiver.StopReceiver$@4600b781,false) from Actor[akka://sparkDriver/deadLetters]
INFO Executor task launch worker-0 org.apache.spark.streaming.receiver.ReceiverSupervisorImpl - Stopped receiver without error
INFO Executor task launch worker-0 org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 915 bytes result sent to driver
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(StatusUpdate(0,FINISHED,java.nio.HeapByteBuffer[pos=0 lim=915 cap=915]),false) from Actor[akka://sparkDriver/deadLetters]
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(StatusUpdate(0,FINISHED,java.nio.HeapByteBuffer[pos=0 lim=915 cap=915]),false)
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0, runningTasks: 0
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_293, runningTasks: 1
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.18856 ms) AkkaMessage(StatusUpdate(0,FINISHED,java.nio.HeapByteBuffer[pos=890 lim=915 cap=915]),false) from Actor[akka://sparkDriver/deadLetters]
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1365300 ms on localhost (1/1)
INFO dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (start at SparkProcess.java:99) finished in 1365.328 s
INFO task-result-getter-0 org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
DEBUG dag-scheduler-event-loop org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 1
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(AllReceiverIds,true) from Actor[akka://sparkDriver/temp/$57b]
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(AllReceiverIds,true)
DEBUG sparkDriver-akka.actor.default-dispatcher-3 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.69631 ms) AkkaMessage(AllReceiverIds,true) from Actor[akka://sparkDriver/temp/$57b]
INFO Thread-9 org.apache.spark.streaming.scheduler.ReceiverTracker - All of the receivers have deregistered successfully
INFO Thread-9 org.apache.spark.streaming.scheduler.ReceiverTracker - ReceiverTracker stopped
INFO Thread-9 org.apache.spark.streaming.scheduler.JobGenerator - Stopping JobGenerator immediately
INFO Thread-9 org.apache.spark.streaming.util.RecurringTimer - Stopped timer for JobGenerator after time 1463343411000
INFO Thread-9 org.apache.spark.streaming.scheduler.JobGenerator - Stopped JobGenerator
DEBUG Thread-9 org.apache.spark.streaming.scheduler.JobScheduler - Stopping job executor
DEBUG cluster-ClusterId{value='5738d441e5203708e7533387', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d441e5203708e7533387', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.3 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d825e5203708e7533bb0', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3318}
DEBUG cluster-ClusterId{value='5738d825e5203708e7533bb0', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3318}
DEBUG Thread-1-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0325, packet:: clientPath:null serverPath:null finished:false header:: 523,5  replyHeader:: 523,17843,0  request:: '/consumers/testgroup/offsets/mongoDB/0,#36343033,-1  response:: s{122,17843,1463242097365,1463343411539,1913,0,0,0,4,0,122} 
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mytopic
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,1] has leader 0
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mytopic are 0,1
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mytopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6534 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6534 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic sparktopic
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,1] has leader 0
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342040192-a08aa80e-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mytopic:0: fetched offset = 10043: consumed offset = 10042) to 10043
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: sparktopic are 0,1
DEBUG mytopic-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mytopic:0: fetched offset = 10043: consumed offset = 10043 to 10043
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [sparktopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6534 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAddCookies - CookieSpec selected: best-match
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAuthCache - Auth cache not set in the context
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection request: [route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection leased: [id: 0][route: {}->http://localhost:8888][total kept alive: 0; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Stale connection check
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6534 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[read] I/O error: Read timed out"
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Executing request POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Target auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Proxy auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Length: 192
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Host: localhost:8888
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Connection: Keep-Alive
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> User-Agent: Apache-HttpClient/4.3.3 (java 1.5)
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Accept-Encoding: gzip,deflate
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "POST /api/v1/datapoints HTTP/1.1[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Length: 192[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Host: localhost:8888[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Connection: Keep-Alive[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "User-Agent: Apache-HttpClient/4.3.3 (java 1.5)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Accept-Encoding: gzip,deflate[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[{"name":"28-ff-2c-31-44-04-00-c2","tags":{"deviceType":"DS18B20","celciusTemperature":"18.5","temperature":"65.3","deviceId":"28-ff-2c-31-44-04-00-c2"},"datapoints":[[1463343411605,"65.3"]]}]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "HTTP/1.1 204 No Content[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Access-Control-Allow-Origin: *[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Pragma: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Cache-Control: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Expires: 0[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Server: Jetty(8.1.16.v20140903)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[\r][\n]"
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << HTTP/1.1 204 No Content
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Access-Control-Allow-Origin: *
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Pragma: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Cache-Control: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Expires: 0
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Server: Jetty(8.1.16.v20140903)
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Connection can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection [id: 0][route: {}->http://localhost:8888] can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection released: [id: 0][route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mongoDB
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,0] has leader 0
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,1] has leader 0
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mongoDB are 0,1
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mongoDB,0]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6534 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6534 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342041123-51e37853-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mongoDB:0: fetched offset = 6404: consumed offset = 6403) to 6404
DEBUG mongoDB-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mongoDB:0: fetched offset = 6404: consumed offset = 6404 to 6404
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {listCollections : BsonInt32{value=1}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {getMore : BsonInt64{value=5848473492}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Inserting 1 documents into namespace idea.devices on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Insert completed
DEBUG cluster-ClusterId{value='5738d3f1e5203708e753324d', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d3f1e5203708e753324d', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.2 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3319}
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3319}
DEBUG Thread-1-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0323, packet:: clientPath:null serverPath:null finished:false header:: 549,5  replyHeader:: 549,17844,0  request:: '/consumers/testgroup/offsets/mytopic/0,#3130303433,-1  response:: s{97,17844,1463242096736,1463343411742,2535,0,0,0,5,0,97} 
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(293)
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaning accumulator 293
INFO Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaned accumulator 293
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(291)
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaning broadcast 291
DEBUG Spark Context Cleaner org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 291
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(RemoveBroadcast(291,true),true) from Actor[akka://sparkDriver/temp/$67b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(RemoveBroadcast(291,true),true)
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(RemoveBroadcast(291,true),true) from Actor[akka://sparkDriver/temp/$77b]
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(RemoveBroadcast(291,true),true)
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.107313 ms) AkkaMessage(RemoveBroadcast(291,true),true) from Actor[akka://sparkDriver/temp/$67b]
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.083776 ms) AkkaMessage(RemoveBroadcast(291,true),true) from Actor[akka://sparkDriver/temp/$77b]
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 291
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManager - Removing broadcast 291
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManager - Removing block broadcast_291_piece0
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.MemoryStore - Block broadcast_291_piece0 of size 1544 dropped from memory (free 1030399010)
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),broadcast_291_piece0,StorageLevel(false, false, false, false, 1),0,0,0),true) from Actor[akka://sparkDriver/temp/$87b]
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),broadcast_291_piece0,StorageLevel(false, false, false, false, 1),0,0,0),true)
INFO sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_291_piece0 on localhost:54845 in memory (size: 1544.0 B, free: 982.7 MB)
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_291_piece0
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.078462 ms) AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),broadcast_291_piece0,StorageLevel(false, false, false, false, 1),0,0,0),true) from Actor[akka://sparkDriver/temp/$87b]
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManager - Told master about block broadcast_291_piece0
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManager - Removing block broadcast_291
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.MemoryStore - Block broadcast_291 of size 2624 dropped from memory (free 1030401634)
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 291, response is 2
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 2 to AkkaRpcEndpointRef(Actor[akka://sparkDriver/temp/$77b])
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaned broadcast 291
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(292)
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaning accumulator 292
INFO Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaned accumulator 292
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(292)
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaning broadcast 292
DEBUG Spark Context Cleaner org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 292
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(RemoveBroadcast(292,true),true) from Actor[akka://sparkDriver/temp/$97b]
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(RemoveBroadcast(292,true),true)
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(RemoveBroadcast(292,true),true) from Actor[akka://sparkDriver/temp/$+7b]
DEBUG sparkDriver-akka.actor.default-dispatcher-14 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.055261 ms) AkkaMessage(RemoveBroadcast(292,true),true) from Actor[akka://sparkDriver/temp/$97b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(RemoveBroadcast(292,true),true)
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.0441 ms) AkkaMessage(RemoveBroadcast(292,true),true) from Actor[akka://sparkDriver/temp/$+7b]
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 292
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManager - Removing broadcast 292
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManager - Removing block broadcast_292
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.MemoryStore - Block broadcast_292 of size 2624 dropped from memory (free 1030404258)
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManager - Removing block broadcast_292_piece0
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.MemoryStore - Block broadcast_292_piece0 of size 1545 dropped from memory (free 1030405803)
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] received message AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),broadcast_292_piece0,StorageLevel(false, false, false, false, 1),0,0,0),true) from Actor[akka://sparkDriver/temp/$~7b]
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - Received RPC message: AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),broadcast_292_piece0,StorageLevel(false, false, false, false, 1),0,0,0),true)
INFO sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.storage.BlockManagerInfo - Removed broadcast_292_piece0 on localhost:54845 in memory (size: 1545.0 B, free: 982.7 MB)
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_292_piece0
DEBUG block-manager-slave-async-thread-pool-28 org.apache.spark.storage.BlockManager - Told master about block broadcast_292_piece0
DEBUG sparkDriver-akka.actor.default-dispatcher-2 org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1 - [actor] handled message (0.06974 ms) AkkaMessage(UpdateBlockInfo(BlockManagerId(driver, localhost, 54845),broadcast_292_piece0,StorageLevel(false, false, false, false, 1),0,0,0),true) from Actor[akka://sparkDriver/temp/$~7b]
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 292, response is 2
DEBUG block-manager-slave-async-thread-pool-23 org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 2 to AkkaRpcEndpointRef(Actor[akka://sparkDriver/temp/$+7b])
DEBUG Spark Context Cleaner org.apache.spark.ContextCleaner - Cleaned broadcast 292
DEBUG cluster-ClusterId{value='5738d3e7e5203708e753321d', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d3e7e5203708e753321d', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.3 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d42ee5203708e7533338', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d42ee5203708e7533338', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.3 ms, state=CONNECTED}]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mytopic
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,1] has leader 0
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mytopic are 0,1
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mytopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6536 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6536 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic sparktopic
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,1] has leader 0
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: sparktopic are 0,1
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [sparktopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6536 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342040192-a08aa80e-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mytopic:0: fetched offset = 10044: consumed offset = 10043) to 10044
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6536 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG mytopic-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mytopic:0: fetched offset = 10044: consumed offset = 10044 to 10044
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAddCookies - CookieSpec selected: best-match
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAuthCache - Auth cache not set in the context
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection request: [route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection leased: [id: 0][route: {}->http://localhost:8888][total kept alive: 0; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Stale connection check
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[read] I/O error: Read timed out"
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Executing request POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Target auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Proxy auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Length: 2
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Host: localhost:8888
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Connection: Keep-Alive
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> User-Agent: Apache-HttpClient/4.3.3 (java 1.5)
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Accept-Encoding: gzip,deflate
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "POST /api/v1/datapoints HTTP/1.1[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Length: 2[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Host: localhost:8888[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Connection: Keep-Alive[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "User-Agent: Apache-HttpClient/4.3.3 (java 1.5)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Accept-Encoding: gzip,deflate[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "HTTP/1.1 204 No Content[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Access-Control-Allow-Origin: *[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Pragma: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Cache-Control: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Expires: 0[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Server: Jetty(8.1.16.v20140903)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[\r][\n]"
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << HTTP/1.1 204 No Content
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Access-Control-Allow-Origin: *
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Pragma: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Cache-Control: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Expires: 0
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Server: Jetty(8.1.16.v20140903)
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Connection can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection [id: 0][route: {}->http://localhost:8888] can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection released: [id: 0][route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mongoDB
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,0] has leader 0
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,1] has leader 0
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mongoDB are 0,1
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mongoDB,0]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6536 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6536 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342041123-51e37853-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mongoDB:0: fetched offset = 6405: consumed offset = 6404) to 6405
DEBUG mongoDB-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mongoDB:0: fetched offset = 6405: consumed offset = 6405 to 6405
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {listCollections : BsonInt32{value=1}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {getMore : BsonInt64{value=7915359377}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Inserting 1 documents into namespace idea.devices on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Insert completed
DEBUG cluster-ClusterId{value='5738d410e5203708e75332c2', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d410e5203708e75332c2', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.4 ms, state=CONNECTED}]
DEBUG cluster-ClusterId{value='5738d410e5203708e75332c1', description='null'}-localhost:27017 org.mongodb.driver.cluster - Checking status of localhost:27017
DEBUG cluster-ClusterId{value='5738d410e5203708e75332c1', description='null'}-localhost:27017 org.mongodb.driver.cluster - Updating cluster description to  {type=STANDALONE, servers=[{address=localhost:27017, type=STANDALONE, roundTripTime=0.4 ms, state=CONNECTED}]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mytopic
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [mytopic,1] has leader 0
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mytopic are 0,1
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mytopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6538 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6538 for topics [mytopic,0] to broker 0 on localhost:9092
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic sparktopic
DEBUG mytopic-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mytopic:0: fetched offset = 10045: consumed offset = 10045 to 10045
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342040192-a08aa80e-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mytopic:0: fetched offset = 10045: consumed offset = 10044) to 10045
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,0] has leader 0
DEBUG Thread-0 kafka.producer.BrokerPartitionInfo - Partition [sparktopic,1] has leader 0
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: sparktopic are 0,1
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [sparktopic,0]
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6538 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAddCookies - CookieSpec selected: best-match
DEBUG mytopic-0 org.apache.http.client.protocol.RequestAuthCache - Auth cache not set in the context
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection request: [route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection leased: [id: 0][route: {}->http://localhost:8888][total kept alive: 0; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Stale connection check
DEBUG Thread-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6538 for topics [sparktopic,0] to broker 0 on localhost:9092
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[read] I/O error: Read timed out"
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Executing request POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Target auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Proxy auth state: UNCHALLENGED
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> POST /api/v1/datapoints HTTP/1.1
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Length: 173
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Host: localhost:8888
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Connection: Keep-Alive
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> User-Agent: Apache-HttpClient/4.3.3 (java 1.5)
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 >> Accept-Encoding: gzip,deflate
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "POST /api/v1/datapoints HTTP/1.1[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Length: 173[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Host: localhost:8888[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Connection: Keep-Alive[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "User-Agent: Apache-HttpClient/4.3.3 (java 1.5)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "Accept-Encoding: gzip,deflate[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 >> "[{"name":"28-26-1c-60-07-00-00-ad","tags":{"deviceType":"DS18B20","celciusTemperature":"17.87","deviceId":"28-26-1c-60-07-00-00-ad"},"datapoints":[[1463343412058,"64.18"]]}]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "HTTP/1.1 204 No Content[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Access-Control-Allow-Origin: *[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Pragma: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Cache-Control: no-cache[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Expires: 0[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Content-Type: application/json; charset=UTF-8[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "Server: Jetty(8.1.16.v20140903)[\r][\n]"
DEBUG mytopic-0 org.apache.http.wire - http-outgoing-0 << "[\r][\n]"
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << HTTP/1.1 204 No Content
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Access-Control-Allow-Origin: *
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Pragma: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Cache-Control: no-cache
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Expires: 0
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Content-Type: application/json; charset=UTF-8
DEBUG mytopic-0 org.apache.http.headers - http-outgoing-0 << Server: Jetty(8.1.16.v20140903)
DEBUG mytopic-0 org.apache.http.impl.execchain.MainClientExec - Connection can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection [id: 0][route: {}->http://localhost:8888] can be kept alive indefinitely
DEBUG mytopic-0 org.apache.http.impl.conn.PoolingHttpClientConnectionManager - Connection released: [id: 0][route: {}->http://localhost:8888][total kept alive: 1; route allocated: 1 of 2; total allocated: 1 of 20]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Handling 1 events
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Getting broker partition info for topic mongoDB
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,0] has leader 0
DEBUG mytopic-0 kafka.producer.BrokerPartitionInfo - Partition [mongoDB,1] has leader 0
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Broker partitions registered for topic: mongoDB are 0,1
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Sending 1 messages with no compression to [mongoDB,0]
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sending messages with correlation id 6538 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG mytopic-0 kafka.producer.async.DefaultEventHandler - Producer sent messages with correlation id 6538 for topics [mongoDB,0] to broker 0 on localhost:9092
DEBUG ConsumerFetcherThread-testgroup_Surbhis-MacBook-Pro.local-1463342041123-51e37853-0-0 kafka.consumer.PartitionTopicInfo - updated fetch offset of (mongoDB:0: fetched offset = 6406: consumed offset = 6405) to 6406
DEBUG mongoDB-0 kafka.consumer.PartitionTopicInfo - reset consume offset of mongoDB:0: fetched offset = 6406: consumed offset = 6406 to 6406
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {listCollections : BsonInt32{value=1}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Sending command {getMore : BsonInt64{value=5861089008}} to database idea on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.command - Command execution completed
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Inserting 1 documents into namespace idea.devices on connection [connectionId{localValue:2, serverValue:5113}] to server localhost:27017
DEBUG mongoDB-0 org.mongodb.driver.protocol.insert - Insert completed
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3320}
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3320}
DEBUG cluster-ClusterId{value='5738d6aae5203708e7533749', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3321}
DEBUG cluster-ClusterId{value='5738d6aae5203708e7533749', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3321}
DEBUG cluster-ClusterId{value='5738d768e5203708e7533943', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3322}
DEBUG cluster-ClusterId{value='5738d768e5203708e7533943', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3322}
DEBUG Thread-1-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Got ping response for sessionid: 0x154b0e6621f0324 after 0ms
DEBUG Thread-1-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0325, packet:: clientPath:null serverPath:null finished:false header:: 524,5  replyHeader:: 524,17845,0  request:: '/consumers/testgroup/offsets/mongoDB/0,#36343036,-1  response:: s{122,17845,1463242097365,1463343412539,1914,0,0,0,4,0,122} 
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3323}
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3323}
DEBUG Thread-1-SendThread(localhost:2181) org.apache.zookeeper.ClientCnxn - Reading reply sessionid:0x154b0e6621f0323, packet:: clientPath:null serverPath:null finished:false header:: 550,5  replyHeader:: 550,17846,0  request:: '/consumers/testgroup/offsets/mytopic/0,#3130303435,-1  response:: s{97,17846,1463242096736,1463343412742,2536,0,0,0,5,0,97} 
DEBUG cluster-ClusterId{value='5738d903e5203708e7533e7e', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3324}
DEBUG cluster-ClusterId{value='5738d903e5203708e7533e7e', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3324}
DEBUG cluster-ClusterId{value='5738d845e5203708e7533c10', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3325}
DEBUG cluster-ClusterId{value='5738d845e5203708e7533c10', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3325}
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3326}
DEBUG cluster-ClusterId{value='5738d922e5203708e7533ede', description='null'}-localhost:27017 org.mongodb.driver.connection - Closing connection connectionId{localValue:3326}
DEBUG Thread-9 org.apache.spark.streaming.scheduler.JobScheduler - Stopped job executor
INFO Thread-9 org.apache.spark.streaming.scheduler.JobScheduler - Stopped JobScheduler
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container org.spark-project.jetty.server.handler.ContextHandlerCollection@4b135c6e - o.s.j.s.ServletContextHandler{/streaming,null} as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/streaming,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@79404f5f
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@79404f5f
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$1-464dc9d4
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$1-464dc9d4
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@79404f5f
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/streaming,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/streaming,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/streaming,null} - org.spark-project.jetty.servlet.ServletHandler@79404f5f as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/streaming,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container org.spark-project.jetty.server.handler.ContextHandlerCollection@4b135c6e - o.s.j.s.ServletContextHandler{/streaming/batch,null} as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/streaming/batch,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@272e56d1
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@272e56d1
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$1-316f3d9c
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$1-316f3d9c
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@272e56d1
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/streaming/batch,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/streaming/batch,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/streaming/batch,null} - org.spark-project.jetty.servlet.ServletHandler@272e56d1 as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/streaming/batch,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container org.spark-project.jetty.server.handler.ContextHandlerCollection@4b135c6e - o.s.j.s.ServletContextHandler{/static/streaming,null} as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/static/streaming,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@34a6711e
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@34a6711e
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.DefaultServlet-ff844f9
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.DefaultServlet-ff844f9
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@34a6711e
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/static/streaming,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/static/streaming,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/static/streaming,null} - org.spark-project.jetty.servlet.ServletHandler@34a6711e as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/static/streaming,null}
INFO Thread-9 org.apache.spark.streaming.StreamingContext - StreamingContext stopped successfully
INFO Thread-9 org.apache.spark.SparkContext - Invoking stop() from shutdown hook
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.server.Server@415917f7
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping SelectChannelConnector@0.0.0.0:4040
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@5ae591e3
DEBUG qtp2001119456-91 Selector0 org.spark-project.jetty.io.nio - Stopped Thread[qtp2001119456-91 Selector0,5,main] on org.spark-project.jetty.io.nio.SelectorManager$1@39381774
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.server.nio.SelectChannelConnector$ConnectorSelectorManager@5ae591e3
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping PooledBuffers [0/1024@6144,0/1024@16384,0/1024@-]/PooledBuffers [0/1024@6144,0/1024@32768,0/1024@-]
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED null/null
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED SelectChannelConnector@0.0.0.0:4040
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.ContextHandlerCollection@4b135c6e
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.server.handler.ContextHandlerCollection@4b135c6e
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/streaming/batch/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@6950fe51
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@6950fe51
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$1-2d784fe6
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$1-2d784fe6
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@6950fe51
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/streaming/batch/json,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/streaming/batch/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/streaming/batch/json,null} - org.spark-project.jetty.servlet.ServletHandler@6950fe51 as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/streaming/batch/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/streaming/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@6db510dd
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@6db510dd
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$1-4d9bdc8
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$1-4d9bdc8
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@6db510dd
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/streaming/json,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/streaming/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/streaming/json,null} - org.spark-project.jetty.servlet.ServletHandler@6db510dd as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/streaming/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@7e306bad
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@7e306bad
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$1-792275e2
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$1-792275e2
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@7e306bad
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/metrics/json,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/metrics/json,null} - org.spark-project.jetty.servlet.ServletHandler@7e306bad as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/metrics/json,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@656ceabd
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@ee8e5d2
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@ee8e5d2
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-46b1e253
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-46b1e253
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@ee8e5d2
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/stages/stage/kill,null} - org.spark-project.jetty.servlet.ServletHandler@ee8e5d2 as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@656ceabd
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@656ceabd
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@1260fda1
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/api,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@77706dde
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@77706dde
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping com.sun.jersey.spi.container.servlet.ServletContainer-30f52163
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED com.sun.jersey.spi.container.servlet.ServletContainer-30f52163
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@77706dde
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/api,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/api,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/api,null} - org.spark-project.jetty.servlet.ServletHandler@77706dde as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/api,null}
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@1260fda1
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@1260fda1
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@5ea7acd8
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@5a6df709
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@5a6df709
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.apache.spark.ui.JettyUtils$$anon$2-207b6036
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.apache.spark.ui.JettyUtils$$anon$2-207b6036
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.servlet.ServletHandler@5a6df709
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping o.s.j.s.ServletContextHandler{/,null}
INFO Thread-9 org.spark-project.jetty.server.handler.ContextHandler - stopped o.s.j.s.ServletContextHandler{/,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.Container - Container o.s.j.s.ServletContextHandler{/,null} - org.spark-project.jetty.servlet.ServletHandler@5a6df709 as handler
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED o.s.j.s.ServletContextHandler{/,null}
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.server.handler.GzipHandler@5ea7acd8
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - STOPPED org.spark-project.jetty.server.handler.GzipHandler@5ea7acd8
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.server.handler.GzipHandler@40cdd385
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping o.s.j.s.ServletContextHandler{/static,null}
DEBUG Thread-9 org.spark-project.jetty.util.component.AbstractLifeCycle - stopping org.spark-project.jetty.servlet.ServletHandler@cfdae24
DEBUG Thread-9 org.spark-project.jetty.server.handler.AbstractHandler - stopping org.spark-project.jetty.servlet.ServletHandler@cfdae24
